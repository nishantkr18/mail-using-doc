{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documents are loaded into memory\n",
    "\"\"\"\n",
    "\n",
    "from typing import List\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def load_data() -> List[Document]:\n",
    "    \"\"\"\n",
    "    The function that loads the data.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader('./docs/', glob=\"**/*.txt\", loader_cls=TextLoader,\n",
    "                            loader_kwargs={'autodetect_encoding': True}, \n",
    "                            )\n",
    "    try:\n",
    "        docs = loader.load()\n",
    "        print(f\"{len(docs)} documents loaded.\")\n",
    "    except:\n",
    "        print(\"Error loading documents.\")\n",
    "        raise\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the code to:\n",
    "1. Split the data into chunks (sentences).\n",
    "2. Create vector embeddings of these sentences.\n",
    "3. Store them in a vectorstore.\n",
    "\"\"\"\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "\n",
    "\n",
    "def process_data(docs: List[Document]):\n",
    "    \"\"\"\n",
    "    The function that processes the data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split into sentences\n",
    "    source_chunks = []\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\".\", chunk_size=500, chunk_overlap=0)\n",
    "    for source in docs:\n",
    "        for chunk in splitter.split_text(source.page_content):\n",
    "            source_chunks.append(\n",
    "                Document(page_content=chunk, metadata=source.metadata))\n",
    "\n",
    "    print('chunks created: ', len(source_chunks))\n",
    "\n",
    "    # Create vector embeddings and store in vectorstore.\n",
    "    print('Creating embeddings...')\n",
    "    embedding = HuggingFaceEmbeddings()\n",
    "\n",
    "    print('Creating vectorstore...')\n",
    "\n",
    "    client = chromadb.Client(Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=\"./.vectorstore\"\n",
    "    ))\n",
    "    client.persist()\n",
    "\n",
    "    # Cleaning up the client\n",
    "    client.reset()\n",
    "\n",
    "    vectorstore = Chroma(client=client)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=source_chunks, embedding=embedding, client=client)\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The main bot file.\n",
    "\"\"\"\n",
    "\n",
    "import textwrap\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from openai.error import AuthenticationError\n",
    "\n",
    "\n",
    "class Bot:\n",
    "    \"\"\"\n",
    "    The bot class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm = OpenAI(temperature=0, max_tokens=200, verbose=True)\n",
    "        self.chain = None\n",
    "\n",
    "    def ask(self, vectorstore, question):\n",
    "        \"\"\"\n",
    "        Asking the bot a question.\n",
    "        \"\"\"\n",
    "\n",
    "        # Using the LLMChain to make your own prompt.\n",
    "        prompt = PromptTemplate(template=textwrap.dedent(\"\"\"\n",
    "        Provided is the name of a user, followed by a question.\n",
    "        Write a short, crisp mail, answering the question based on the information provided in the context.\n",
    "        Make sure to exclude any information that is not relevant to the question.\n",
    "        Do not hallucinate. If you cant find the answer in the context, mention that you dont know.\n",
    "        #####\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\"\"\"), input_variables=['context', 'question'])\n",
    "\n",
    "        most_relevant_source = vectorstore.similarity_search(question, k=1)[\n",
    "            0].metadata['source']\n",
    "\n",
    "        self.chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm, chain_type='stuff', retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4, \"filter\": {'source': most_relevant_source}}),\n",
    "            chain_type_kwargs={\"verbose\": True, \"prompt\": prompt})\n",
    "\n",
    "        with get_openai_callback() as callback:\n",
    "            try:\n",
    "                response = self.chain.run(question)\n",
    "            except AuthenticationError:\n",
    "                response = \"Invalid API key.\"\n",
    "            print(callback)\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1103, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "2 documents loaded.\n",
      "chunks created:  11\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ./.vectorstore\n",
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vectorstore...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print('Running...')\n",
    "\n",
    "# Loading environment variables for OpenAI.\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "docs = load_data()\n",
    "\n",
    "vectorstore = process_data(docs)\n",
    "\n",
    "bot = Bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"travis may: Is or will the app be compatible with the Samsung health app?\"\n",
    "question2 = \"Sofia: Hi, How can I change the calories goal? Thanks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provided is the name of a user, followed by a question.\n",
      "Write a short, crisp mail, answering the question based on the information provided in the context.\n",
      "Make sure to exclude any information that is not relevant to the question.\n",
      "Do not hallucinate. If you cant find the answer in the context, mention that you dont know.\n",
      "#####\n",
      "\n",
      "Context:\n",
      "Currently, the following information is imported from Fitbit devices into the app:\n",
      "\n",
      "- Steps\n",
      "- Calories burned on activities\n",
      "\n",
      "The following information is exported from the app to your Fitbit account:\n",
      "\n",
      "- Foods consumed with the number of calories\n",
      "- Water consumed\n",
      "\n",
      "\n",
      "The app can be connected to Fibit, but this option is only available in the Plus version of the app.\n",
      "Offer the user the code CODE1MONTH to try the Fitbit connection for free\n",
      "\n",
      "The promo codes can be used inside the \"My account\" section of the setting menu of the app, using the option \"I have a code\". Only registered users can redeem codes so they must register an account first if they have not yet done so.\n",
      "Google Fit and Samsung Health are not supported at the moment. But we will work on it.\n",
      "\n",
      "How to solve this? It is recommended to set your profile activity level to “*Low*” if you have an Apple Watch connected. That way, the app will not increase your caloric budget and it will only increase with the calories burned imported from your Apple Watch\n",
      "\n",
      "With an Apple Watch connected to the app (through the Health app), the calories that you burn on daily movement will be added to the app as exercise, increasing your caloric budget for the day. The problem is that the app also increases your caloric budget based on the activity level selected on your profile, therefore you will be double-counting burned calories on daily activity\n",
      "\n",
      "Question:\n",
      "travis may: Is or will the app be compatible with the Samsung health app?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Tokens Used: 467\n",
      "\tPrompt Tokens: 417\n",
      "\tCompletion Tokens: 50\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.009340000000000001\n",
      "\n",
      "\n",
      "Hi Travis,\n",
      "\n",
      "Thank you for your question. Unfortunately, the app is not currently compatible with Samsung Health. We are working on it, however, so please stay tuned for updates.\n",
      "\n",
      "Regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "response = bot.ask(vectorstore=vectorstore, question=question1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi Travis,\n",
      "\n",
      "Thank you for your question. Unfortunately, the app is not currently compatible with Samsung Health. We are working on it, however, so please stay tuned for updates.\n",
      "\n",
      "Regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
